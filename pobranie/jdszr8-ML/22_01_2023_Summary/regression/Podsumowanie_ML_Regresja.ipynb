{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Przewidywanie-zanieczyszczenia-powietrza-(PM2.5)-w-Warszawie\" data-toc-modified-id=\"Przewidywanie-zanieczyszczenia-powietrza-(PM2.5)-w-Warszawie-1\">Przewidywanie zanieczyszczenia powietrza (PM2.5) w Warszawie</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-1.1\">Dataset</a></span></li><li><span><a href=\"#1.-Zrozumienie-biznesu\" data-toc-modified-id=\"1.-Zrozumienie-biznesu-1.2\">1. Zrozumienie biznesu</a></span></li><li><span><a href=\"#2.-Zrozumienie-danych\" data-toc-modified-id=\"2.-Zrozumienie-danych-1.3\">2. Zrozumienie danych</a></span><ul class=\"toc-item\"><li><span><a href=\"#Darksky\" data-toc-modified-id=\"Darksky-1.3.1\">Darksky</a></span></li><li><span><a href=\"#GIOS\" data-toc-modified-id=\"GIOS-1.3.2\">GIOS</a></span></li></ul></li><li><span><a href=\"#Preprocessing-danych\" data-toc-modified-id=\"Preprocessing-danych-1.4\">Preprocessing danych</a></span></li><li><span><a href=\"#Radzenie-sobie-z-brakujacymi-danymi\" data-toc-modified-id=\"Radzenie-sobie-z-brakujacymi-danymi-1.5\">Radzenie sobie z brakujacymi danymi</a></span><ul class=\"toc-item\"><li><span><a href=\"#Usuwanie-cech,-w-ktorych-brakuje-ponad-20%-danych\" data-toc-modified-id=\"Usuwanie-cech,-w-ktorych-brakuje-ponad-20%-danych-1.5.1\">Usuwanie cech, w ktorych brakuje ponad 20% danych</a></span></li><li><span><a href=\"#Uzupelnianie-brakujacych-danych\" data-toc-modified-id=\"Uzupelnianie-brakujacych-danych-1.5.2\">Uzupelnianie brakujacych danych</a></span></li><li><span><a href=\"#Cechy,-ktore-moga-sie-przydac-podczas-EDA\" data-toc-modified-id=\"Cechy,-ktore-moga-sie-przydac-podczas-EDA-1.5.3\">Cechy, ktore moga sie przydac podczas EDA</a></span></li></ul></li><li><span><a href=\"#2.1-EDA\" data-toc-modified-id=\"2.1-EDA-1.6\">2.1 EDA</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Czyszczenie-outliers-dla-target-variable-(PM2.5)\" data-toc-modified-id=\"Czyszczenie-outliers-dla-target-variable-(PM2.5)-1.6.0.1\">Czyszczenie outliers dla target variable (PM2.5)</a></span></li></ul></li></ul></li><li><span><a href=\"#3.-Wybor-metryki-sukcesu-i-walidacji\" data-toc-modified-id=\"3.-Wybor-metryki-sukcesu-i-walidacji-1.7\">3. Wybor metryki sukcesu i walidacji</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Najbardziej-znane-metryki-dla-regresji-(przypomnienie):\" data-toc-modified-id=\"Najbardziej-znane-metryki-dla-regresji-(przypomnienie):-1.7.0.1\">Najbardziej znane metryki dla regresji (przypomnienie):</a></span></li></ul></li></ul></li><li><span><a href=\"#4.-Budowa-prostego-modelu\" data-toc-modified-id=\"4.-Budowa-prostego-modelu-1.8\">4. Budowa prostego modelu</a></span><ul class=\"toc-item\"><li><span><a href=\"#Przed-u≈ºyciem-plot_feature_importances-trzeba-zrobiƒá-fit\" data-toc-modified-id=\"Przed-u≈ºyciem-plot_feature_importances-trzeba-zrobiƒá-fit-1.8.1\">Przed u≈ºyciem plot_feature_importances trzeba zrobiƒá fit</a></span></li></ul></li><li><span><a href=\"#5.-Feature-Engineering\" data-toc-modified-id=\"5.-Feature-Engineering-1.9\">5. Feature Engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Don‚Äôt-Miss-Out-on-Rolling-Window-Functions-in-Pandas\" data-toc-modified-id=\"Don‚Äôt-Miss-Out-on-Rolling-Window-Functions-in-Pandas-1.9.1\"><a href=\"https://towardsdatascience.com/dont-miss-out-on-rolling-window-functions-in-pandas-850b817131db\" target=\"_blank\">Don‚Äôt Miss Out on Rolling Window Functions in Pandas</a></a></span></li></ul></li><li><span><a href=\"#6.-Bardziej-zaawansowane-modele\" data-toc-modified-id=\"6.-Bardziej-zaawansowane-modele-1.10\">6. Bardziej zaawansowane modele</a></span></li><li><span><a href=\"#7.-Wybor-cech\" data-toc-modified-id=\"7.-Wybor-cech-1.11\">7. Wybor cech</a></span></li></ul></li><li><span><a href=\"#Dla-zainteresowanych\" data-toc-modified-id=\"Dla-zainteresowanych-2\">Dla zainteresowanych</a></span><ul class=\"toc-item\"><li><span><a href=\"#8.-Optimalizacja-HyperOpt'em\" data-toc-modified-id=\"8.-Optimalizacja-HyperOpt'em-2.1\">8. Optimalizacja <a href=\"https://hyperopt.github.io/hyperopt/?source=post_page\" target=\"_blank\">HyperOpt'em</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#TPE---Tree-structured-Parzen-Estimator\" data-toc-modified-id=\"TPE---Tree-structured-Parzen-Estimator-2.1.1\"><a href=\"https://optunity.readthedocs.io/en/latest/user/solvers/TPE.html#:~:text=The%20Tree%2Dstructured%20Parzen%20Estimator,test%20based%20on%20this%20model.\" target=\"_blank\">TPE - Tree-structured Parzen Estimator</a></a></span></li></ul></li><li><span><a href=\"#Dzielimy-dane,-ustalamy-walidacje-i-metryki-sukcesu\" data-toc-modified-id=\"Dzielimy-dane,-ustalamy-walidacje-i-metryki-sukcesu-2.2\">Dzielimy dane, ustalamy walidacje i metryki sukcesu</a></span></li><li><span><a href=\"#Odpalamy-HyperOpt'a,-trenujemy-i-testujemy\" data-toc-modified-id=\"Odpalamy-HyperOpt'a,-trenujemy-i-testujemy-2.3\">Odpalamy HyperOpt'a, trenujemy i testujemy</a></span></li><li><span><a href=\"#Sprawdzamy-wyniki-predykcji-na-poszczeg√≥lnych-miesiƒÖcach\" data-toc-modified-id=\"Sprawdzamy-wyniki-predykcji-na-poszczeg√≥lnych-miesiƒÖcach-2.4\">Sprawdzamy wyniki predykcji na poszczeg√≥lnych miesiƒÖcach</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przewidywanie zanieczyszczenia powietrza (PM2.5) w Warszawie\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Dane meteo pochodzƒÖ z [darksky.net API](https://darksky.net/dev/docs#time-machine-request). (Apple ich wykupi≈Ç jaki≈õ rok temu i ju≈º dostƒôpu nie ma üò• ) \n",
    "<br>Dane odno≈õnie zanieczyszczenia powietrza w Warszawie z [GIO≈ö Archives](http://powietrze.gios.gov.pl/pjp/archives)\n",
    "\n",
    "\n",
    "**Features**\n",
    "<br>The darksky dataset consists of approximately 43,500 data points, with each datapoint having 20 features. \n",
    "\n",
    "- `apparentTemperature`: the apparent (or ‚Äúfeels like‚Äù) temperature in degrees Fahrenheit.\n",
    "- `cloudCover`: the percentage of sky occluded by clouds, between 0 and 1, inclusive.\n",
    "- `dewpoint`: the dew point in degrees Fahrenheit.\n",
    "- `humidity`: the relative humidity, between 0 and 1, inclusive.\n",
    "- `icon`: a machine-readable text summary of this data point, suitable for selecting an icon for display.\n",
    "- `ozone`: the columnar density of total atmospheric ozone at the given time in Dobson units.\n",
    "- `precipAccumulation`: the amount of snowfall accumulation expected to occur in inches. If no snowfall is expected, this property will not be defined.\n",
    "- `precipIntensity`: the intensity (in inches of liquid water per hour) of precipitation occurring at the given time. This value is conditional on probability (that is, assuming any precipitation occurs at all).\n",
    "- `precipProbability`: the probability of precipitation occurring, between 0 and 1, inclusive.\n",
    "- `precipType`: the type of precipitation occurring at the given time. If precipIntensity is zero, then this property will not be defined.\n",
    "- `pressure`: the sea-level air pressure in millibars.\n",
    "- `summary`: a human-readable text summary of this data point. This property has millions of possible values, so don‚Äôt use it for automated purposes: use the icon property, instead.\n",
    "- `temperature`: the air temperature in degrees Fahrenheit.\n",
    "- `time`\n",
    "- `uvIndex`: the UV index.            \n",
    "- `visibility`: the average visibility in miles, capped at 10 miles.          \n",
    "- `windBearing`: the direction that the wind is coming from in degrees, with true north at 0¬∞ and progressing clockwise. If windSpeed is zero, then this value will not be defined.\n",
    "- `windGust`: the wind gust speed in miles per hour.\n",
    "- `windspeed`: the wind speed in miles per hour.\n",
    "\n",
    "**Target Variable**\n",
    "<br>The target variable comes from the GIOS dataset which has approximately 43,500 data points, with each datapoint having 4 features:\n",
    "\n",
    "- `pm25_nie`: Warsaw, Niepodleg≈Ço≈õci\n",
    "- `pm25_kon`: Warsaw, Kondratowicza\n",
    "- `pm25_wok`: Warsaw, Wokalna\n",
    "- `date`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Zrozumienie biznesu\n",
    "\n",
    "Nic nowego, ze Polska jest w ogonie Europy jesli chodzi o jako≈õƒá powietrza. Celem projektu jest dok≈Çadne przewidzenie jednostek py≈Çu PM2.5 w powietrzu na dany dzie≈Ñ i godzinƒô, tak by ludzie mogli zdecydowaƒá, czy za≈Ço≈ºyƒá maskƒô, otworzyƒá okno, by wywietrzyƒá mieszkanie, czy dobrym pomys≈Çem jest p√≥j≈õƒá na spacer.\n",
    "\n",
    "![reg_metrics](img/air_quality.png)\n",
    "\n",
    "https://airindex.eea.europa.eu/Map/AQI/Viewer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Zrozumienie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "base_color = sns.color_palette()[0]\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "random_state=2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darksky = pd.read_csv('./data/data_darksky_hourly_2015-2019.csv')\n",
    "gios = pd.read_excel('./data/2015-2019_PM25_1g.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Darksky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darksky.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darksky.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darksky.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcja sprawdzajƒÖca brakujƒÖce warto≈õci\n",
    "def check_missing(dataframe):\n",
    "    for column in dataframe.columns:\n",
    "        missing = column, dataframe[column].isnull().sum()\n",
    "        if missing[1] == 0: continue\n",
    "        print(missing)\n",
    "        \n",
    "check_missing(darksky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcja rysujƒÖca wykres s≈Çupkowy dla brakujƒÖcych warto≈õci\n",
    "def plot_missing(dataframe, cutoff):\n",
    "    missing_perc =( (dataframe.isnull().sum()/dataframe.shape[0]) * 100 ).sort_values(ascending=False)\n",
    "    plt.title(\"Missing values [%]\", y=1.015)\n",
    "    missing_perc.plot.bar(figsize=(10,5), color=base_color);\n",
    "    plt.hlines(xmin=dataframe.index.min(), xmax=dataframe.index.max(), y=cutoff, color='r', linestyle='-.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing(darksky, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# podczas sprawdzania danych okaza≈Ço siƒô, ≈ºe jest r√≥≈ºnica czasowa miedzy GIOS i darksky dataset\n",
    "# tak≈ºe trzeba to zmieniƒá.\n",
    "darksky['datetime'] = pd.to_datetime(darksky.time + 3600, unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quant_features(dataframe, black_list):\n",
    "    feats = dataframe.select_dtypes([np.number, np.bool]).columns\n",
    "    return [x for x in feats if x not in black_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdzamy rozk≈Çad danych w czasie\n",
    "black_list = ['Unnamed: 0', 'time']\n",
    "darksky_feats = get_quant_features(darksky, black_list)\n",
    "\n",
    "i = 1\n",
    "\n",
    "plt.figure(figsize=(10, 14))\n",
    "for x in darksky_feats:\n",
    "    plt.subplot(len(darksky_feats), 1, i)\n",
    "    darksky[x].plot()\n",
    "    plt.title(f\"{x}\", y=0.5, loc='right')\n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcja rysujƒÖca histogramy\n",
    "def draw_histograms(df, variables, n_rows, n_cols):\n",
    "    fig=plt.figure(figsize=(16,8))\n",
    "    for i, var_name in enumerate(variables):\n",
    "        ax=fig.add_subplot(n_rows, n_cols, i+1)\n",
    "        df[var_name].hist(bins=50,ax=ax)\n",
    "        ax.set_title(var_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_histograms(darksky, darksky_feats, 4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gios.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gios.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gios.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 rekord√≥w zawiera milisekundy (r√≥≈ºnica formatu), trzeba to zmieniƒá\n",
    "gios['datetime'] = gios['datetime'].astype('datetime64[s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing(gios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing(gios, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdzamy rozk≈Çad PM2.5 (target variable) w czasie\n",
    "gios_feats = ['MzWarAlNiepo', 'MzWarKondrat', 'MzWarWokalna']\n",
    "\n",
    "i = 1\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for x in gios_feats:\n",
    "    plt.subplot(len(gios_feats), 1, i)\n",
    "    gios[x].plot()\n",
    "    plt.title(f\"{x}\", y=0.5, loc='right')\n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_histograms(gios, gios_feats, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing danych\n",
    "\n",
    "Przed przystƒÖpieniem do tego zadania warto po≈ÇƒÖczyƒá oba zbiory danych. Jak my≈õlicie, jakie mogƒÖ byƒá problemy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe z datami i godzinami miedzy 2015-01-01 00:00:00' a '2019-12-31 23:00:00\n",
    "df_dates = pd.DataFrame(pd.date_range(start='2015-01-01', end='2020-01-01', freq='H'), columns=['datetime'])\n",
    "df_dates.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# po≈ÇƒÖczenie zbior√≥w\n",
    "df = pd.merge(df_dates, darksky, on=['datetime'], how='left')\n",
    "df = pd.merge(df, gios, on=['datetime'], how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usuwamy zbƒôdne kolumny\n",
    "df.drop(['Unnamed: 0', 'summary', 'time'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zmiana nazwy kolumn dla target variables\n",
    "df.rename(columns={'MzWarAlNiepo':'PM25_nie', 'MzWarKondrat':'PM25_kon', 'MzWarWokalna':'PM25_wok'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usuniƒôcie 1:00AM z  01/01/2020\n",
    "df = df[:-1]\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing(df, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radzenie sobie z brakujacymi danymi\n",
    "Dodatkowo polecam:\n",
    "- https://towardsdatascience.com/all-about-missing-data-handling-b94b8b5d2184"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usuwanie cech, w ktorych brakuje ponad 20% danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_perc =( (df.isnull().sum()/df.shape[0]) * 100 ).sort_values(ascending=False)\n",
    "outliers = [col for col in df.columns if missing_perc[col] > 20]\n",
    "\n",
    "df = df.drop(outliers, axis=1)\n",
    "\n",
    "# dodatkowo pozbywamy sie PM2.5 dla Wokalnej\n",
    "df = df.drop('PM25_wok', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uzupelnianie brakujacych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dla 'icon' zamieniamy Null na 'unknown'\n",
    "df['icon'] = df['icon'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Interpolation**\n",
    "- Interpolacja liniowa jest czƒôsto stosowana do przybli≈ºenia warto≈õci jakiej≈õ funkcji poprzez wykorzystanie dw√≥ch znanych warto≈õci tej funkcji w innych punktach. Wz√≥r ten mo≈ºe byƒá r√≥wnie≈º rozumiany jako ≈õrednia wa≈ºona. Wagi sƒÖ odwrotnie zwiƒÖzane z odleg≈Ço≈õciƒÖ od punkt√≥w ko≈Ñcowych do nieznanego punktu. Punkt bli≈ºszy ma wiƒôkszy wp≈Çyw ni≈º punkt dalszy. \n",
    "- MajƒÖc do czynienia z brakujƒÖcymi danymi, powiniene≈õ u≈ºyƒá tej metody w szeregu czasowym, kt√≥ry wykazuje liniƒô trendu, ale nie bƒôdzie ona odpowiednia dla danych sezonowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats=['apparentTemperature','cloudCover', 'dewPoint', 'humidity', 'precipIntensity',\n",
    "      'precipProbability', 'pressure', 'temperature', 'visibility', 'windBearing', \n",
    "      'windSpeed', 'PM25_nie']\n",
    "\n",
    "for feat in feats:\n",
    "    df[feat] = df[feat].interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/python-pandas-dataframe-ffill/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backfill dla brakujƒÖcych warto≈õci\n",
    "feats=['cloudCover', 'precipIntensity','precipProbability', \n",
    "       'pressure', 'visibility', 'windBearing', 'windSpeed']\n",
    "\n",
    "for feat in feats:\n",
    "    df[feat] = df[feat].fillna(method ='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forwardfill dla brakujƒÖcych warto≈õci\n",
    "df['uvIndex'] = df['uvIndex'].fillna(method ='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cechy, ktore moga sie przydac podczas EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dni, godziny, tygodnie\n",
    "df['hour'] = df.datetime.dt.hour\n",
    "df['day'] = df.datetime.dt.day\n",
    "df['dayofyear'] = df.datetime.dt.dayofyear\n",
    "df['weekday'] = df.datetime.dt.weekday\n",
    "df['month'] = df.datetime.dt.month\n",
    "df['year'] = df.datetime.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# czy dany dzie≈Ñ jest weekendem?\n",
    "df['IsWeekend'] = df['weekday'].apply(lambda x: 0 if x <5 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pory roku\n",
    "seasons = ['Winter', 'Winter', 'Spring', 'Spring', 'Spring', 'Summer',\n",
    "           'Summer', 'Summer', 'Autumn', 'Autumn', 'Autumn', 'Winter']\n",
    "month_to_season = dict(zip(range(1, 13), seasons))\n",
    "\n",
    "df['season'] = df.month.map(month_to_season)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 EDA\n",
    "przyklady"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "plt.rcParams['figure.figsize']=(15,10)\n",
    "%time ax = sns.heatmap(df[df.columns[:-10]].corr(), vmax=1., vmin=-1., annot=True, linewidths=.8, cmap=\"YlGnBu\")\n",
    "plt.title(\"Correlation matrix\", fontsize=15, y=1.025);\n",
    "\n",
    "# poprawka na matplotliba by nie ciƒÖ≈Ç brzeg√≥w wykresu\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdzamy, czy sƒÖ jakie≈õ warto≈õci odstajƒÖce\n",
    "# pd.diff() sprawdza r√≥≈ºnice co do poprzedniego elementu\n",
    "plt.plot(df.index, df['PM25_nie'].diff(), 'bo', alpha=0.3, ms=10, lw='3')\n",
    "\n",
    "plt.title(\"Niepodleglosci Avenue PM2.5 point differences\", y=1.015)\n",
    "plt.hlines(xmin=df.index.min(), xmax=df.index.max(), y=100, color='r', linestyle='-.')\n",
    "plt.hlines(xmin=df.index.min(), xmax=df.index.max(), y=-100, color='r', linestyle='-.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"PM2.5 level over years and months\", y=1.015)\n",
    "sns.barplot(x='month', y='PM25_nie', hue='year', data=df)\n",
    "plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 8))\n",
    "\n",
    "# heatmap dla ≈õredniej PM2.5 dla godzin i miesiƒôcy\n",
    "df_train = df.groupby([\"month\", \"hour\"])[\"PM25_nie\"].mean().reset_index()\n",
    "df_train = df_train.pivot(\"month\", \"hour\", \"PM25_nie\")\n",
    "ax = sns.heatmap(df_train, cmap=\"BuPu\", square=True)\n",
    "\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "\n",
    "plt.title(\"Mean PM2.5 Heatmap per hour and month\", y=1.015)\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Month');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 8))\n",
    "\n",
    "# heatmap dla ≈õredniej temperatury dla godzin i miesiƒôcy\n",
    "df_train = df.groupby([\"month\", \"hour\"])[\"temperature\"].mean().reset_index()\n",
    "df_train = df_train.pivot(\"month\", \"hour\", \"temperature\")\n",
    "ax = sns.heatmap(df_train, cmap=\"BuPu\", square=True)\n",
    "\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "\n",
    "plt.title(\"Mean TEMP Heatmap per hour and month\", y=1.015)\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Month');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 8))\n",
    "\n",
    "# heatmap dla ≈õredniej wilgotno≈õci dla godzin i miesiƒôcy\n",
    "df_train = df.groupby([\"month\", \"hour\"])[\"humidity\"].mean().reset_index()\n",
    "df_train = df_train.pivot(\"month\", \"hour\", \"humidity\")\n",
    "ax = sns.heatmap(df_train, cmap=\"BuPu\", square=True)\n",
    "\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "\n",
    "plt.title(\"Mean Humidity Heatmap per hour and month\", y=1.015)\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Month');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Czyszczenie outliers dla target variable (PM2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r√≥≈ºnice w PM2.5 co do godziny\n",
    "df['PM25_nie_diff'] = df['PM25_nie'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# znalezienie 'id' dla r√≥≈ºnic powy≈ºej 100 punkt√≥w PM2.5\n",
    "df[['datetime', 'PM25_nie', 'PM25_nie_diff']].query('PM25_nie_diff > 100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdzenie co jest dooko≈Ça tych outliers, mo≈ºe majƒÖ prawo bytu...\n",
    "df[['datetime', 'PM25_nie', 'PM25_nie_diff']].query('index > 24665 and index < 24680')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['datetime', 'PM25_nie', 'PM25_nie_diff']].query('index > 28700 and index < 28710')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zamiana na NAs\n",
    "df.loc[(df['PM25_nie_diff'] > 100), 'PM25_nie']  = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uzupe≈Çnienie przy u≈ºyciu linear interpolation\n",
    "df['PM25_nie'] = df['PM25_nie'].interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdzenie wynik√≥w\n",
    "plt.plot(df.index, df['PM25_nie'].diff(), 'bo', alpha=0.3, ms=10, lw='3')\n",
    "\n",
    "plt.title(\"PM2.5 point differences\", y=1.015)\n",
    "plt.hlines(xmin=df.index.min(), xmax=df.index.max(), y=100, color='r', linestyle='-.')\n",
    "plt.hlines(xmin=df.index.min(), xmax=df.index.max(), y=-100, color='r', linestyle='-.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Wybor metryki sukcesu i walidacji\n",
    "\n",
    "\n",
    "#### Najbardziej znane metryki dla regresji (przypomnienie):\n",
    "* [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error) (mse)\n",
    "* [mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error) (mae)\n",
    "* [root mean square error](https://en.wikipedia.org/wiki/Root-mean-square_deviation) (rmse)\n",
    "* [root mean square logarithm error](https://www.slideshare.net/KhorSoonHin/rmsle-cost-function) (rmsle)\n",
    "* [R-squared](https://en.wikipedia.org/wiki/Coefficient_of_determination) (R<sup>2</sup>)\n",
    "\n",
    "<br>Dodatkowo:\n",
    "* [mae vs rmse](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d)\n",
    "\n",
    "![reg_metrics](img/reg_metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![metryki](img/metryki.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "- https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Budowa prostego modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# walidacja (mozna sprawdzic na Kfold jak i na TimeSeriesSplit)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "\n",
    "# podzial na train/test\n",
    "train_mask = df['datetime'] < '2019-07-01'\n",
    "test_mask = df['datetime'] >= '2019-07-01'\n",
    "\n",
    "df_train = df.loc[train_mask]\n",
    "df_test = df.loc[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, X, y, cross_val):\n",
    "    model_name = type(model).__name__\n",
    "\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, cv=cross_val, scoring='neg_root_mean_squared_error'))\n",
    "    r2 = cross_val_score(model, X, y, cv=cross_val, scoring='r2')\n",
    "    print(\"{} rmse: {:.4f}, r2: {:.1f}%\".format(model_name, np.mean(rmse), 100*np.mean(r2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "def plot_feature_importances(model, feat_list, ax):\n",
    "\n",
    "    model_name = type(model).__name__\n",
    "    skplt.estimators.plot_feature_importances(model, feature_names=df[feat_list].columns,\n",
    "                                            title=f'{model_name} feature importances', ax=ax)\n",
    "    plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define simple models for fast testing\n",
    "Dummy_model = DummyRegressor(strategy=\"mean\")\n",
    "DT_model = DecisionTreeRegressor(max_depth=3, random_state=random_state)\n",
    "RF_model = RandomForestRegressor(max_depth=3, random_state=random_state)\n",
    "XGB_model = XGBRegressor(max_depth=3, objective='reg:squarederror', random_state=random_state)\n",
    "\n",
    "# add all models to a list\n",
    "models = [Dummy_model, DT_model, RF_model, XGB_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_list = ['PM25_nie', 'Unnamed: 0', 'PM25_nie_diff']\n",
    "feats = get_quant_features(df, black_list)\n",
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[feats].values\n",
    "y_train = df_train['PM25_nie'].values\n",
    "\n",
    "X_test = df_test[feats].values\n",
    "y_test = df_test['PM25_nie'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    run_model(model, X_train, y_train, kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przed u≈ºyciem plot_feature_importances trzeba zrobiƒá fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit models\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(15,6))\n",
    "for i, model in enumerate(models[1:]):\n",
    "    ax=fig.add_subplot(1,3,i+1)\n",
    "    plot_feature_importances(model, feats, ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "\n",
    "Warto poszukaƒá w internecie, czy kto≈õ ju≈º nie rozwiƒÖzywa≈Ç podobnego problemu i poza pomys≈Çami na transformacje, po≈ÇƒÖczenia, mo≈ºe wpad≈Ç na jaka≈õ 'z≈ÇotƒÖ cechƒô'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przeksztalcenie logarytmiczne (rowniez target variable)\n",
    "log_feats = ['humidity', 'windSpeed', 'PM25_nie']\n",
    "\n",
    "for feat in log_feats:\n",
    "    df['{}_log'.format(feat)] = np.log1p(df[feat].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_feats = ['humidity_log', 'windSpeed_log', 'PM25_nie_log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_histograms(df, log_feats, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przesuniecie o 1h i 24h\n",
    "feats = ['apparentTemperature', 'cloudCover', 'dewPoint', 'humidity', 'humidity_log',\n",
    "         'precipIntensity', 'precipProbability', 'pressure', 'temperature', 'uvIndex',\n",
    "         'visibility', 'windBearing', 'windSpeed', 'windSpeed_log']\n",
    "lags = [1, 24]\n",
    "\n",
    "for lag in lags:\n",
    "    for feat in feats:\n",
    "        df['{0}_lag{1}h'.format(feat, lag)] = df[feat].shift(lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['datetime', 'pressure', 'pressure_lag1h']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backfill brakujacych danych w nowo utworzonych cechach\n",
    "lag_cols = [col for col in df.columns if 'lag' in col]\n",
    "\n",
    "for col in lag_cols:\n",
    "    df[col] = df[col].fillna(method ='bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Don‚Äôt Miss Out on Rolling Window Functions in Pandas](https://towardsdatascience.com/dont-miss-out-on-rolling-window-functions-in-pandas-850b817131db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tworzenie ≈õrednich kroczƒÖcych (w godzinach)\n",
    "rolls = [12, 24, 72, 168]\n",
    "\n",
    "for roll in rolls:\n",
    "    for feat in feats:\n",
    "        df['{0}_rolling{1}h'.format(feat, roll)] = df[feat].rolling(roll).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backfill brakujƒÖcych danych w nowo utworzonych cechach\n",
    "rolling_cols = [col for col in df.columns if 'rolling' in col]\n",
    "\n",
    "for col in rolling_cols:\n",
    "    df[col] = df[col].fillna(method ='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = ['temperature', 'temperature_rolling12h', 'temperature_rolling24h',\n",
    "         'temperature_rolling72h', 'temperature_rolling168h']\n",
    "\n",
    "\n",
    "for x in feats:\n",
    "    df_copy = df.copy()\n",
    "    df_copy.index = df.datetime\n",
    "    plt.title('Temperature Distribution over time')\n",
    "    df_copy[-744:][x].plot(figsize=(16,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_list = ['PM25_nie', 'Unnamed: 0', 'PM25_nie_diff', 'PM25_nie_log']\n",
    "feats = get_quant_features(df, black_list)\n",
    "feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bardziej zaawansowane modele\n",
    "**Tutaj testujemy na nowych cechach modele zdefiniowane powyzej**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# podzial na train/test\n",
    "train_mask = df['datetime'] < '2019-07-01'\n",
    "test_mask = df['datetime'] >= '2019-07-01'\n",
    "\n",
    "df_train = df.loc[train_mask]\n",
    "df_test = df.loc[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[feats]\n",
    "y_train = df_train['PM25_nie']\n",
    "\n",
    "X_test = df_test[feats]\n",
    "y_test = df_test['PM25_nie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    run_model(model, X_train, y_train, kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit models\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(15,6))\n",
    "for i, model in enumerate(models[1:]):\n",
    "    ax=fig.add_subplot(1,3,i+1)\n",
    "    plot_feature_importances(model, feats, ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Wybor cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dla zainteresowanych\n",
    "## 8. Optimalizacja [HyperOpt'em](https://hyperopt.github.io/hyperopt/?source=post_page)\n",
    "\n",
    "[Bardzo dobry artyku≈Ç por√≥wnujƒÖcy GridSearch, RandomSearch i HyperOpt](https://www.vantage-ai.com/en/blog/bayesian-optimization-for-quicker-hyperparameter-tuning)\n",
    "\n",
    "`pip install hyperopt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials, space_eval\n",
    "from hyperopt.pyll.base import scope\n",
    "from hyperopt.pyll.stochastic import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dostepne metody.\n",
    "\n",
    "- **hp.choice**(label, options)\n",
    "Returns one of the options, which should be a list or tuple. The elements of options can themselves be [nested] stochastic expressions. In this case, the stochastic choices that only appear in some of the options become conditional parameters.\n",
    "\n",
    "- **hp.randint**(label, upper)\n",
    "Returns a random integer in the range [0, upper). The semantics of this distribution is that there is no more correlation in the loss function between nearby integer values, as compared with more distant integer values. This is an appropriate distribution for describing random seeds for example. If the loss function is probably more correlated for nearby integer values, then you should probably use one of the \"quantized\" continuous distributions, such as either quniform, qloguniform, qnormal or qlognormal.\n",
    "\n",
    "- **hp.uniform**(label, low, high)\n",
    "Returns a value uniformly between low and high.\n",
    "When optimizing, this variable is constrained to a two-sided interval.\n",
    "\n",
    "- **hp.quniform**(label, low, high, q)\n",
    "Returns a value like round(uniform(low, high) / q) * q\n",
    "Suitable for a discrete value with respect to which the objective is still somewhat \"smooth\", but which should be bounded both above and below.\n",
    "\n",
    "- **hp.loguniform**(label, low, high)\n",
    "Returns a value drawn according to exp(uniform(low, high)) so that the logarithm of the return value is uniformly distributed.\n",
    "When optimizing, this variable is constrained to the interval [exp(low), exp(high)].\n",
    "\n",
    "- **hp.qloguniform**(label, low, high, q)\n",
    "Returns a value like round(exp(uniform(low, high)) / q) * q\n",
    "Suitable for a discrete variable with respect to which the objective is \"smooth\" and gets smoother with the size of the value, but which should be bounded both above and below.\n",
    "\n",
    "- **hp.normal**(label, mu, sigma)\n",
    "Returns a real value that's normally-distributed with mean mu and standard deviation sigma. When optimizing, this is an unconstrained variable.\n",
    "\n",
    "- **hp.qnormal**(label, mu, sigma, q)\n",
    "Returns a value like round(normal(mu, sigma) / q) * q\n",
    "Suitable for a discrete variable that probably takes a value around mu, but is fundamentally unbounded.\n",
    "\n",
    "- **hp.lognormal**(label, mu, sigma)\n",
    "Returns a value drawn according to exp(normal(mu, sigma)) so that the logarithm of the return value is normally distributed. When optimizing, this variable is constrained to be positive.\n",
    "\n",
    "- **hp.qlognormal**(label, mu, sigma, q)\n",
    "Returns a value like round(exp(normal(mu, sigma)) / q) * q\n",
    "Suitable for a discrete variable with respect to which the objective is smooth and gets smoother with the size of the variable, which is bounded from one side.\n",
    "\n",
    "\n",
    "Prefix `q` oznacza dyskretne stany, zamiast ciƒÖg≈Ço≈õci (np. 1, 3, 5, a nie 1.3, 3.4, 5.2), r√≥wnie≈º mo≈ºemy podawaƒá krok pomiƒôdzy liczbami. Na przyk≈Çad, przej≈õcie od 5 do 20 o jeden krok: `hp.quniform ('x_max_depth', 5, 20, 1)`. Kolejno≈õƒá jest losowa i mo≈ºe pojawiƒá sie wiƒôcej ni≈º jeden raz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgb_hyperopt(X, y, cv, max_evals, random_state):\n",
    "    \n",
    "    # funkcja, kt√≥rej wynik chcemy minimalizowaƒá (RMSE)\n",
    "    def objective(space):\n",
    "        XGB_model = XGBRegressor(\n",
    "            max_depth = int(space['max_depth']),\n",
    "            n_estimators = int(space['n_estimators']),\n",
    "            min_child_weight = space['min_child_weight'],\n",
    "            subsample = space['subsample'],\n",
    "            learning_rate = space['learning_rate'],\n",
    "            gamma = space['gamma'],\n",
    "            colsample_bytree = space['colsample_bytree'],\n",
    "            objective='reg:squarederror',\n",
    "            random_state=random_state)\n",
    "        \n",
    "        for train_idx, test_idx in cv.split(X):\n",
    "\n",
    "            eval_set = [(X[train_idx], y[train_idx]), (X[test_idx], y[test_idx])]\n",
    "            XGB_model.fit(X[train_idx], y[train_idx], eval_set=eval_set, eval_metric = 'rmse', early_stopping_rounds=10, verbose=False)\n",
    "            y_pred = XGB_model.predict(X[test_idx])\n",
    "\n",
    "            rmse = np.sqrt(mean_squared_error(y[test_idx], y_pred))\n",
    "            \n",
    "            print(\"RMSE: {:.4f}\".format(rmse))\n",
    "            return{'loss': rmse, 'status': STATUS_OK }\n",
    "    \n",
    "    # parametry do optymizacji    \n",
    "    space ={\n",
    "            'max_depth': hp.choice('max_depth', np.arange(2, 4, 1, dtype=int)),\n",
    "            'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "            'subsample': hp.uniform('subsample', 0.7, 1),\n",
    "            'n_estimators' : hp.choice('n_estimators', np.arange(20, 150, 2, dtype=int)),\n",
    "            'learning_rate' : hp.quniform('learning_rate', 0.025, 0.5, 0.025),\n",
    "            'gamma' : hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "            'colsample_bytree' : hp.quniform('colsample_bytree', 0.5, 1, 0.05)\n",
    "        }\n",
    "\n",
    "    trials = Trials() # przechowuje info dla kazdego kroku.\n",
    "    \n",
    "    best = fmin(fn=objective, #fmin - funkcja, ktora chcemy minimalizowac\n",
    "                space=space, #przestrze≈Ñ potencjalnych (hyper)parametr√≥w\n",
    "                algo=tpe.suggest, #algorytm wyszukujacy najlepsze parametry\n",
    "                max_evals=max_evals, #ilo≈õƒá pr√≥b\n",
    "                trials=trials) #zapisywane wynikow w trakcie\n",
    "    \n",
    "    hyperparams = space_eval(space, best)\n",
    "\n",
    "    print(\"The best params: \", hyperparams)\n",
    "    return hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TPE - Tree-structured Parzen Estimator](https://optunity.readthedocs.io/en/latest/user/solvers/TPE.html#:~:text=The%20Tree%2Dstructured%20Parzen%20Estimator,test%20based%20on%20this%20model.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dzielimy dane, ustalamy walidacje i metryki sukcesu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zdefiniowanie X_train, y_train\n",
    "X_train = df_train[feats].values\n",
    "y_train = df_train['PM25_nie'].values\n",
    "\n",
    "# zdefiniowanie X_test, y_test\n",
    "X_test = df_test[feats].values\n",
    "y_test = df_test['PM25_nie'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odpalamy HyperOpt'a, trenujemy i testujemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odaplenie hyeropt\n",
    "best_XGB = run_xgb_hyperopt(X_train, y_train, cv=kf, max_evals=25, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zainicjowanie modelu z najlepszymi parametrami\n",
    "XGB_model = XGBRegressor(**best_XGB, objective='reg:squarederror', random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trening\n",
    "XGB_model.fit(X_train, y_train)\n",
    "y_pred_XGB = XGB_model.predict(X_test)\n",
    "\n",
    "df_test['PM25_nie_pred'] = y_pred_XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(20,10))\n",
    "ax=fig.add_subplot(2,4,i+1)\n",
    "plot_feature_importances(XGB_model, feats, ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdzenie RMSE\n",
    "xgb_rmse_depth = np.sqrt(mean_squared_error(df_test['PM25_nie'], df_test['PM25_nie_pred']))\n",
    "print(\"XGBoost RMSE: {:.4f}\".format(xgb_rmse_depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprawdzamy wyniki predykcji na poszczeg√≥lnych miesiƒÖcach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = df_test.month.unique().tolist()\n",
    "months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mth in months:\n",
    "    test_mask = df_test['month'] == mth\n",
    "    fig, ax = plt.subplots(figsize=(20,6))\n",
    "    \n",
    "    #forecast\n",
    "    ax.plot(df_test.loc[test_mask]['datetime'], df_test.loc[test_mask]['PM25_nie_pred'],\n",
    "            color='red', linestyle='dashdot', label='XGB PM2.5 Prediction')\n",
    "    #actual\n",
    "    ax.plot(df_test.loc[test_mask]['datetime'], df_test.loc[test_mask]['PM25_nie'],\n",
    "            color='blue', linewidth=2, label='Actual PM2.5') \n",
    "    \n",
    "    legend = ax.legend(loc='upper left', fontsize='medium')\n",
    "    plt.title('PM25_Niepodleg≈Ço≈õci')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
