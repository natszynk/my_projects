{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T13:36:34.871940Z",
     "start_time": "2020-06-06T13:36:34.855939Z"
    },
    "id": "11zYOzUcl5rK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "from IPython.display import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qLP7DE7l5rN"
   },
   "source": [
    "# SVM\n",
    "\n",
    "## 1. Czym są SVM i zasada działania algorytmu\n",
    "## 2. Co to jest Kernel ? \n",
    "## 3. Problem klasyfikacji wielu klas\n",
    "## 4. Hiperparametry modelu\n",
    "***************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBw2UmQfl5rR"
   },
   "source": [
    "# 1. SVM\n",
    "## Suport Vector Machines \n",
    "## Maszyny wektorów wspornych (nośnych) \n",
    "\n",
    "Metoda uczeni maszynowego opracowana w AT&T Bell Laboratories przez Vladimira Vapnika z kolegami w latach 90.\n",
    "\n",
    "Sprowadza się do odpowiedzi na pytanie: jak rozdzielić dwie kategorie używając prostej linii ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T13:36:34.887941Z",
     "start_time": "2020-06-06T13:36:34.873944Z"
    },
    "id": "R2UCxpRZl5rT"
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'Kolor': ['CZ', 'CZ', 'CZ', 'CZ', 'CZ', 'CZ', 'CZ', 'CZ', 'CZ', 'CZ', 'NI', 'NI', 'NI', 'NI', 'NI', 'NI', 'NI', 'NI', 'NI', 'NI', 'NI'],\n",
    "                     'X': [2, 4, 3.5, 4.2, 1, 4.7, 2.4, 1.7, 2.3, 3.7, 5.6, 7, 9.9, 6.8, 5.5, 8.4, 7.2, 6.1, 9, 8.2, 7.9],\n",
    "                     'Y': [1, 2.3, 2, 1.5, 3, 4.8, 2.3, 3.5, 4.6, .9, 10, 8.8, 6.6, 6.8, 7.9, 5.3, 6, 8.7, 9.5, 6.6, 9]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T13:36:35.094531Z",
     "start_time": "2020-06-06T13:36:34.889942Z"
    },
    "id": "9dF0B6sVl5rV",
    "outputId": "b10455e8-65af-4a8b-9088-11e9d8856596",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5));\n",
    "plt.scatter(data[data['Kolor'] =='CZ']['X'], data[data['Kolor'] =='CZ']['Y']);\n",
    "plt.scatter(data[data['Kolor'] =='NI']['X'], data[data['Kolor'] =='NI']['Y']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Fss5Xh4l5rW"
   },
   "source": [
    "Linii rozdzielających dane jak powyżej możemy wyznaczy wiele, w SVM chodzi o to by wybrać tą optymalną.\n",
    "\n",
    "Znajdujemy taką linię, by \"margin\" (odległość między linią klasyfikacji a najbliższymi jej obserwacjami ) było jak największe. \n",
    "\n",
    "Obserwacje leżące najbliżej linii klasyfikacji nazywane są \"support vectors\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8I8OS0-l5rX",
    "outputId": "c7d5de87-aa8f-4974-89db-b8810062a18e"
   },
   "outputs": [],
   "source": [
    "Image(\"img/svm_margin.png\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztXzaVRMl5rY"
   },
   "source": [
    "Gdyby usunąć ze zbioru wszystkie obserwacje poza support vetors to klasyfikator byłby taki sam. Więc dla tego algorytmu istotne są jedynie obserwacje graniczne. Poniżej przestawimy przykład pokazujący to zjawisko. \n",
    "Zatem SVM nie jest tak podatny na outleiery. \n",
    "\n",
    "Zatem widzimy tutaj znaczą różncę względem regresji logistycznej.\n",
    "Ponadto w przypadku SVM otrzymujemy wynik po porstu jako przynależność do klasy, nie mamy tutaj interpretacji probabilistycznej wyniku  \n",
    "\n",
    "Ponadto w przeciwieństwie do regresji logistycznej SVM jest metodą nieparametryczną tzn. nie szukamy tutaj parametrów (wag) dla zmiennych lecz na podstawie zbioru danych będziemy szukać optymalnego podziału klas. \n",
    "\n",
    "**********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T13:36:35.110051Z",
     "start_time": "2020-06-06T13:36:35.097069Z"
    },
    "id": "Ws8NqNQzl5ra"
   },
   "outputs": [],
   "source": [
    "# mapowanie etykiet, tak byśmy potem mogli skorzystać z rysowania wykresów decyzyjnych plot_decision_regions\n",
    "encoder = LabelEncoder()\n",
    "data['Kolor'] = encoder.fit_transform(data['Kolor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T13:36:35.126056Z",
     "start_time": "2020-06-06T13:36:35.111052Z"
    },
    "id": "-4HZ79aXl5rc"
   },
   "outputs": [],
   "source": [
    "# uczenie modelu stadarodow jak innych w pakiecie sklearn\n",
    "# wybieramy kernel liniowy by pokazać podział linią prostą - o krenelach będzie w dalszej części \n",
    "simple_svm = SVC(kernel='linear')\n",
    "simple_svm.fit(data[['X', 'Y']], data['Kolor']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T13:36:35.380562Z",
     "start_time": "2020-06-06T13:36:35.128053Z"
    },
    "id": "DSoJc0Kdl5re",
    "outputId": "ee467921-dbc1-418d-e32b-ca4993892361",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rysowanie wykresu decyzyjności na przetrzeni \n",
    "plot_decision_regions(X = data[['X', 'Y']].to_numpy(), y = data['Kolor'].to_numpy().astype(np.int), clf=simple_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEC6RdSll5rf",
    "outputId": "374fdf14-c997-4a19-c268-43e78b0403fa"
   },
   "outputs": [],
   "source": [
    "data2 = pd.DataFrame({'Kolor': [  'CZ',  'CZ', 'CZ', 'CZ', 'CZ', 'NI', 'NI', 'NI', 'NI', 'NI', 'NI', 'NI'],\n",
    "                     'X': [ 4.2,  4.7, 2.4, 1.7,  3.7, 5.6, 7, 9.9, 6.8, 5.5, 8.4, 7.2],\n",
    "                     'Y': [   1.5, 4.8, 2.3, 3.5, .9, 10, 8.8, 6.6, 6.8, 7.9, 5.3, 6]})\n",
    "\n",
    "data2['Kolor'] = LabelEncoder().fit_transform(data2['Kolor'])\n",
    "simple_svm2 = SVC(kernel='linear').fit(data2[['X', 'Y']], data2['Kolor'])\n",
    "plot_decision_regions(X = data2[['X', 'Y']].to_numpy(), y = data2['Kolor'].to_numpy().astype(np.int), clf=simple_svm2);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcBVi1B6l5rg"
   },
   "source": [
    "## Jak wygląda klasyfikator w przestrzeni ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1o9HJn9Dl5rh",
    "outputId": "e7b522a6-014f-465c-a61e-dd322fef3187"
   },
   "outputs": [],
   "source": [
    "Image(\"img/hiperpłaszczyzna.png\", width=850)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3Z1AXpll5ri"
   },
   "source": [
    "## Miękki margines\n",
    "\n",
    "Dotychczas przedstawiany był twardy margines, tj. żaden punkt w zbiorze treningowym nie znajdował się między liniami wyznaczonymi przez wektory wspierające.\n",
    "Jest to kontrolowane przez parametr\n",
    "C (o hiperparametrach w daszej części).\n",
    "\n",
    "Punkty wewnątrz marginesu uważamy za potencjalnie błędnie zaklasyfikowane\n",
    "\n",
    "*********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmcSamIjl5ri",
    "outputId": "36eacd0e-90dd-44b4-9b9d-e90cc7e09862"
   },
   "outputs": [],
   "source": [
    "Image(\"img/soft_margin.png\", width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cslpT4Nql5rj",
    "outputId": "db341e8b-6fb7-432c-fef0-5aa5260aba98"
   },
   "outputs": [],
   "source": [
    "## w pakiecie dataset mamy wiele funkcji do generowania różnych struktur danych \n",
    "df = datasets.make_blobs(n_samples=200, random_state=67, centers=2,cluster_std=3)\n",
    "X = pd.DataFrame(df[0], columns=['a', 'b'])\n",
    "y = df[1]\n",
    "sns.scatterplot(X.a, X.b, hue = y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Mqhz9Cvl5rk",
    "outputId": "18937b77-d3b4-4b06-c21a-a51c87131812"
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel='linear').fit(X, y)\n",
    "plot_decision_regions(X = X.to_numpy(), y = y, clf=svm);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0hvf-qHl5rk"
   },
   "source": [
    "## SVM dla regresji\n",
    "\n",
    "Zasadę działania SMV możemy również zasosować w problemach regresji. Działanie to jest mniej intuicyjne niż w przypadku klasyfikacji ale zasada również polega na tym, że będzimy chcieli zminimalizować błąd przez znalezienie takiej hiperpłaszczyzy dla któej margines będzie największy. W praktyce chodzi o to by otrzymana linia regresji zawierała jak najwięcej punktów w jak najbliższej odlegości od niej.\n",
    "*****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpLJeFbYl5rl",
    "outputId": "f9e77c69-e0d8-42cd-f108-6a99441ee15d"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/Advertising.csv\")\n",
    "df = df.drop(\"Unnamed: 0\", axis=1)\n",
    "X = df['TV']\n",
    "y = df['Sales']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tM7oNMRGl5rl",
    "outputId": "130d3f1c-dcfb-49b1-bc8d-d97281530bd1"
   },
   "outputs": [],
   "source": [
    "srv_reg = SVR(kernel='linear').fit(X.to_numpy().reshape(-1,1), y)\n",
    "\n",
    "def reg(x):\n",
    "    y = srv_reg.predict(x)\n",
    "    return y\n",
    "\n",
    "xs = np.arange(min(X),max(X),0.01)\n",
    "plt.plot(xs,reg(xs.reshape(-1,1)),color=\"black\");\n",
    "plt.scatter(X,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwvbuZvwl5rn"
   },
   "source": [
    "# 2.Kernel\n",
    "\n",
    "## Mapowanie do wyższych wymiarów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3s3EgRbl5rn",
    "outputId": "f042127e-ab97-4d9f-f241-65832db9c02b"
   },
   "outputs": [],
   "source": [
    "Image(\"img/mapowanie3d.png\", width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8SNiI-fl5ro"
   },
   "source": [
    "## Jak łatwo odseparować od siebie klasy, które wydają się być nie do odseparowania?\n",
    "## Przykład liniowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mIq1nDJl5rp",
    "outputId": "55bad09f-0340-4530-8cca-c417e0f7428c"
   },
   "outputs": [],
   "source": [
    "X = np.linspace(-5, 5, 10)\n",
    "y = np.zeros(10)\n",
    "labs = np.array([0, 0, 0, 0, 1, 1, 0, 0, 0, 0])\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X[labs!=1], y[labs!=1])\n",
    "plt.scatter(X[labs==1], y[labs==1])\n",
    "plt.ylim(bottom=-1, top=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfiSJ-zDl5rq"
   },
   "source": [
    "### Sprowadzenie do wyższego wymiaru przez podniesienie od kwadratu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ld26bm3kl5rq"
   },
   "outputs": [],
   "source": [
    "x_sq = X**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4OQAuD36l5rr",
    "outputId": "143298f8-12f8-410e-93f0-2d9cb1dd5ae2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X[labs!=1], x_sq[labs!=1])\n",
    "plt.scatter(X[labs==1], x_sq[labs==1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atMeW13dl5rs"
   },
   "source": [
    "## Teraz możemy łatwo rozdzielić klasy linią prostą  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EprqltDtl5rt",
    "outputId": "066eb25f-2760-4ff8-8757-d2970c129239"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X[labs!=1], x_sq[labs!=1])\n",
    "plt.scatter(X[labs==1], x_sq[labs==1])\n",
    "plt.axhline(y=1, );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2IvVdDHl5rt"
   },
   "source": [
    "## Kernel (Jądro) jest to funkcja matematyczna przekształcająca przestrzeń obserwacji. Dzięi stosowaniu różnych funkcji możemy przekształcić problemy nieliniowe w liniowo separowalne.\n",
    "\n",
    "Maszyna wektorów nośnych klasyfikuje dane wykorzystując niejawne przekształcenie zbioru treningowego do przestrzeni cech wyższego wymiaru. W nowej przestrzeni cech dopasowywana jest optymalna hiperpłaszczyzna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWBHXKF9l5ru",
    "outputId": "546c8ba8-2a4f-4c7f-f966-7d9a7b7c402b"
   },
   "outputs": [],
   "source": [
    "Image(\"img/kernel.png\", width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0WgFj32l5rv"
   },
   "source": [
    "# Typy kerneli - przykłady na różnych strukturach danych "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6P-Y-2el5rv",
    "outputId": "175fb89f-fadc-42b4-fc3e-298abb739bde"
   },
   "outputs": [],
   "source": [
    "df = datasets.make_circles(n_samples=500,random_state=0, noise =.2, factor=.1)\n",
    "X = pd.DataFrame(df[0], columns=['a', 'b'])\n",
    "y = df[1]\n",
    "sns.scatterplot(X.a, X.b, hue = y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRWNBNsnl5rw"
   },
   "source": [
    "## Liniowy (linear)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T13:36:36.157057Z",
     "start_time": "2020-06-06T13:36:35.922632Z"
    },
    "id": "gO7DW51Ql5ry",
    "outputId": "bfbacc70-1191-4593-d123-656071bf58d7"
   },
   "outputs": [],
   "source": [
    "lin_svm = SVC(kernel='linear',gamma = 'scale')\n",
    "lin_svm.fit(X,y)\n",
    "plot_decision_regions(X.to_numpy(),y, clf=lin_svm);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQamk7Gal5ry"
   },
   "source": [
    "## Wilomianowy (poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_IpWuN5l5ry"
   },
   "source": [
    "### Degree = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T13:36:36.378925Z",
     "start_time": "2020-06-06T13:36:36.158835Z"
    },
    "id": "Cybpl_8ml5rz",
    "outputId": "a93aa380-8e87-42b3-d428-f1637f25a9f7"
   },
   "outputs": [],
   "source": [
    "svm_poly2 = SVC(kernel='poly', degree=2,gamma = 'scale').fit(X,y)\n",
    "plot_decision_regions(X.to_numpy(),y, clf=svm_poly2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huHkPRekl5rz"
   },
   "source": [
    "### Degree = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T13:36:36.917183Z",
     "start_time": "2020-06-06T13:36:36.381932Z"
    },
    "id": "DV1BNsiOl5r0",
    "outputId": "15cbc643-caf8-49de-ac3f-75eff5f277e6"
   },
   "outputs": [],
   "source": [
    "svm_poly3 = SVC(kernel='poly', degree=3,gamma = 'scale').fit(X,y)\n",
    "plot_decision_regions(X.to_numpy(),y, clf=svm_poly3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDNAqb5il5r1"
   },
   "source": [
    "## Radial (rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T13:36:37.251327Z",
     "start_time": "2020-06-06T13:36:36.919185Z"
    },
    "id": "dawmbzOhl5r1",
    "outputId": "6fb7914d-b045-4a2f-dc9a-fa1d1e36ec04"
   },
   "outputs": [],
   "source": [
    "svm_rad = SVC(kernel='rbf',gamma = 'scale').fit(X,y)\n",
    "plot_decision_regions(X.to_numpy(),y, clf=svm_rad);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4E_iROfl5r2"
   },
   "source": [
    "## Zadanie 1. \n",
    "\n",
    "Dla zbioru danych pniżej stwórz modele SVM z kernelem liniowym, radial i polynominal w kilku wariantach stopni wielomianu.\n",
    "Narysuj wykresy oraz policz skuteczność modeli. Który był najskuteczniejszy ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFtCNG4Wl5r3",
    "outputId": "918a6743-1547-4f12-d1e7-5979ce2cca26"
   },
   "outputs": [],
   "source": [
    "df = datasets.make_moons(n_samples=500, random_state=0, noise = .09)\n",
    "X = pd.DataFrame(df[0], columns=['a', 'b'])\n",
    "y = df[1]\n",
    "sns.scatterplot(X.a, X.b, hue = y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUkSs46el5r3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7RWwoRDl5r3"
   },
   "source": [
    "# 3. Problem klasyfikacji wielu klas\n",
    "\n",
    "SVM jest klasyfikatorem binarnym. Możemy jednak zastosować techniki trenowania wielu klasyfikatorów i łączenia ich w celu uzyskania złożnoego modelu obsługującego wiele klas.\n",
    "\n",
    "Są dwa główne podejścia:\n",
    "- one vs one (stosowany domyślnie w sklearn)\n",
    "- one vs all (aby go użyć należy posłużyć się modelem ```sklearn.multiclass.OneVsRestClassifier```)\n",
    "\n",
    "W pierwszym podejściu budujemy klasyfikatory dla wszystkich możliwych par klas. Jest to z reguły skuteczniejsza metoda lecz złożoność obliczeniowa rośnie nam wykładniczo wraz ze wzrostem liczby klas. \n",
    "W drugim podejściu budujemy jden klasyfikator dla każdej klasy. \n",
    "*******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1hwHU7Jl5r4",
    "outputId": "addda0e0-ebd2-4551-9444-dcd32b654f66"
   },
   "outputs": [],
   "source": [
    "df = datasets.make_blobs(n_samples=500, random_state=67, centers=3,cluster_std=2)\n",
    "X = pd.DataFrame(df[0], columns=['a', 'b'])\n",
    "y = df[1]\n",
    "sns.scatterplot(X.a, X.b, hue = y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkQwRRNFl5r4",
    "outputId": "da05398d-d378-4347-809b-745db87fa851"
   },
   "outputs": [],
   "source": [
    "lin_ovo = SVC(kernel='linear')\n",
    "lin_ovo.fit(X,y)\n",
    "plot_decision_regions(X.to_numpy(),y, clf=lin_ovo);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSRUrBU2l5r5",
    "outputId": "45625bc7-c372-4b82-c1c1-ddc2c5104a69"
   },
   "outputs": [],
   "source": [
    "lin_ovr = OneVsRestClassifier(SVC(kernel='linear')).fit(X,y)\n",
    "plot_decision_regions(X.to_numpy(),y, clf=lin_ovr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Anbep581l5r5"
   },
   "source": [
    "# 4. Hiperparmetry\n",
    "\n",
    "Hiperparamerami nazywamy takie parametry, które definiujemy jeszcze przed rozpoczęciem trenowania. Hiperparametry nie są wyliczane przez algorytm podczas uczenia tak jak np wagi w modelu regresji liniowej. Hiperpaarametry decydują zwykle o tym w jaki sposób ma działać model jakiego rodzaju obliczenia czy przekształecenia ma zastosować, ile iteracji ma wykonać itd. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPyTKM3-l5r6"
   },
   "source": [
    "## Regularyzacja (C)\n",
    "\n",
    "Określa jak bardzo chcemy uniknać złego sklasyfikowania obserwacji. \n",
    "Jeżeli C jest duże, wówczas algorytm dobierze węższe marginesy. Dopasowanie będzie lesze ale możemy mieć prblem z overfttingiem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OQsQnPSl5r6",
    "outputId": "36ca6a32-36e9-4bba-be98-ec5ba316e3bf"
   },
   "outputs": [],
   "source": [
    "df = datasets.make_blobs(n_samples=300, random_state=67, centers=2,cluster_std=2.5)\n",
    "X = pd.DataFrame(df[0], columns=['a', 'b'])\n",
    "y = df[1]\n",
    "sns.scatterplot(X.a, X.b, hue = y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUVyr6ggl5r6",
    "outputId": "2fdc873e-968a-426d-ae99-07fdcc05c085"
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel='rbf',gamma='auto',C=0.1)\n",
    "svm.fit(X,y)\n",
    "plt.title('C = 0.1')\n",
    "plot_decision_regions(X.to_numpy(),y, clf=svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9FG1eHNl5r7",
    "outputId": "ac9883d0-1053-4ec4-bd5d-653f3af4ebbd"
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel='rbf',gamma='auto',C=100)\n",
    "svm.fit(X,y)\n",
    "plt.title('C = 100')\n",
    "plot_decision_regions(X.to_numpy(),y, clf=svm);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeIgLIP5l5r8"
   },
   "source": [
    "## Gamma $\\gamma$\n",
    "\n",
    "Parametr dla kerneli typu radial, poly i sigmoiod. Określa ile oberwacji będzie miało wpływ na na hiperpłaszczyznę.\n",
    "Wysoka wartość parametru oznacza, że tylko kilka punktów będzie miało wpły na podział, więc przestrzeń będzie można \"łatwiej wygiąć\" \n",
    "\n",
    "Możemy też, skorzystać z wartości 'auto' oraz 'scale' - wtedy w zależności od zbioru danych algorytm dobierze nam odpowiednią wartość parametru.\n",
    "Dokumentacja: \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvMJrB9xl5r9",
    "outputId": "8c64ea87-6468-4c2e-c302-f4180b2bd6e1"
   },
   "outputs": [],
   "source": [
    "Image(\"img/gamma.png\", width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1JDd-A8l5r-",
    "outputId": "9812c357-3a1d-4649-f31d-5fdee7911734"
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel='rbf',gamma=0.05)\n",
    "svm.fit(X,y)\n",
    "plt.title('Gamma = 0.05')\n",
    "plot_decision_regions(X.to_numpy(),y, clf=svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tjXRg-3l5r_",
    "outputId": "06d780f4-a2bf-4490-f6ab-9dfb1be84afc"
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel='rbf',gamma=2)\n",
    "svm.fit(X,y)\n",
    "plt.title('Gamma = 2')\n",
    "plot_decision_regions(X.to_numpy(),y, clf=svm);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xo6yzMO8l5r_"
   },
   "source": [
    "## Zastosowanie w klasyfikacji obrazów\n",
    "\n",
    "SVM ze względu na swoją dobrą skuteczność w rozwiązywaniu problemów nieliniowych swojego czasu był wykorzystywany w problemach związanych z klasyfikacją obrazów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RU6rwePKl5sA",
    "outputId": "370f34c2-612b-4116-dc5f-22c804333fb2"
   },
   "outputs": [],
   "source": [
    "mnist = pd.read_csv('data/mnist_test.csv')\n",
    "## bieżemy tylko pierwsze 2000 obseracji żeby szybciej się liczyło\n",
    "mnist = mnist.iloc[0:2000]\n",
    "\n",
    "Y = mnist['label']\n",
    "X = mnist.drop(columns = 'label').to_numpy()\n",
    "print(len(X[0]))\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ma_Dk_TQl5sA",
    "outputId": "3e4c52ff-ad63-46d0-eb85-e5e1490cde3d"
   },
   "outputs": [],
   "source": [
    "plt.matshow(X[0].reshape(28,28));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H74svLwal5sC"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.3,random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weVg7cmBl5sC",
    "outputId": "17d032bd-d4ad-4503-f11b-a108e2e93537"
   },
   "outputs": [],
   "source": [
    "## regresja liniowa \n",
    "lin = LogisticRegression().fit(X_train,Y_train)\n",
    "pred = lin.predict(X_test)\n",
    "print('Accuracy:',accuracy_score(Y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "laoGYo_wl5sD",
    "outputId": "2daed73a-d566-430c-f91c-f551f25b51ea"
   },
   "outputs": [],
   "source": [
    "# SVM\n",
    "svm = SVC(kernel='poly',gamma='auto',degree=3).fit(X_train,Y_train)\n",
    "pred = svm.predict(X_test)\n",
    "print('Accuracy:',accuracy_score(Y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DGGT_kxxl5sD",
    "outputId": "6d784692-896d-4fea-9723-8cab307a8275"
   },
   "outputs": [],
   "source": [
    "confusion_matrix(Y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7PlUPpQl5sD"
   },
   "source": [
    "## Zadanie 2\n",
    "\n",
    "Zbiór mnist (pierwszych 2000 obserwacji) podziel na zbiory: treningowy walidacyjny, testowy w stosunku 60%, 20% 20%. \n",
    "Następnie spróbuj znaleźć najlepszy model SVM. Przetestuj różne kernele i ich hiperparametry (C, gamma) sprawdzając skuteczność (accuracy score) na zbiorze walidacyjnym.\n",
    "Wybierz najlepszy model i na końcu policz skuteczność tego modelu na zbiorze testowym \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5EiOpBBl5sE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
