{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWlRDOHY1CIw"
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "![Logistic Regression](https://drive.google.com/uc?export=view&id=1_zRhqUmMdznHJNGWQuH050wvr-5n_2WS)\n",
    "\n",
    "**[Click Here](https://youtu.be/l8VEth6leXA) Fajne YT video.**\n",
    "\n",
    "W statystyce regresja logistyczna służy do modelowania prawdopodobieństwa wystąpienia określonej klasy lub zdarzenia. \n",
    "\n",
    "Regresja logistyczna jest podobna do regresji liniowej, ponieważ obie polegają na oszacowaniu wartości parametrów użytych w równaniu predykcji na podstawie danych uczących. Regresja liniowa przewiduje wartość pewnej ciągłej zmiennej zależnej. Podczas gdy regresja logistyczna przewiduje prawdopodobieństwo zdarzenia lub klasy zależnej od innych czynników. Zatem wynik regresji logistycznej zawsze mieści się w przedziale od 0 do 1. Ze względu na tę właściwość jest powszechnie używany do celów klasyfikacji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkKuz_oeyuTI"
   },
   "source": [
    "\n",
    "## Logistic Model\n",
    "Mając zmienne *x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub> … x<sub>n</sub>*. \n",
    "\n",
    "Niech wyjście binarne oznaczymy przez *Y*, które może przyjmować wartości 0 lub 1.\n",
    "Niech *p* będzie prawdopodobieństwem *Y = 1*, możemy to oznaczyć jako *p = P(Y=1)*.\n",
    "Matematyczny związek między tymi zmiennymi można zapisać jako:\n",
    "\n",
    "\n",
    "$$ ln(\\frac{p}{1-p}) = b_0 + b_1x_1 + b_2x_2 + b_3x_3 ... b_nx_n $$\n",
    "\n",
    "Termin $\\frac{p}{1-p}$ jest znany jako *odds/szanse* i oznacza prawdopodobieństwo zajścia zdarzenia.\n",
    "\n",
    "Jednak $ln(\\frac{p}{1-p})$ jest znany jako *log odds/logarytm szans* i jest po prostu używany do odwzorowania prawdopodobieństwa, które leży między 0 a 1, na zakres pomiędzy  $(-\\infty, +\\infty)$. Wyrażenia $b_0, b_1, b_2 ...$ to parametry (lub wagi), które będziemy oszacować podczas treningu.\n",
    "\n",
    "To to tylko podstawowa matematyka kryjąca się za tym, co zamierzamy zrobić. Interesuje nas prawdopodobieństwo *p* w tym równaniu. Upraszczamy więc równanie, aby otrzymać wartość p:\n",
    "\n",
    "\n",
    "1. Termin logarytm $ln$ mozna usunac podnoszac do potegi  $e$:  \n",
    "$$ \\frac{p}{1-p}  = e^{b_0 + b_1x_1 + b_2x_2 + b_3x_3 ... b_nx_n} $$\n",
    "2. Teraz możemy łatwo uprościć, aby uzyskać wartość $p$:  \n",
    "$$ p = \\frac{e^{b_0 + b_1x_1 + b_2x_2 + b_3x_3 ... b_nx_n}}{1 + e^{b_0 + b_1x_1 + b_2x_2 + b_3x_3 ... b_nx_n}} $$\n",
    "$$ albo $$\n",
    "$$ p = \\frac{1}{1 + e^{-(b_0 + b_1x_1 + b_2x_2 + b_3x_3 ... b_nx_n)}} $$   \n",
    "\n",
    "W rzeczywistości okazuje się, że jest to równanie *funkcji sigmoidalnej*, która jest szeroko stosowana w innych aplikacjach uczenia maszynowego. *Funkcja sigmoidalna* jest dana wzorem:\n",
    "$$ S(x) = \\frac{1}{1+e^{-x}} $$\n",
    "\n",
    "Teraz będziemy używać powyższego równania wyprowadzonego do naszych przewidywań. Wcześniej wytrenujemy nasz model, aby uzyskać wartości naszych parametrów $b_0, b_1, b_2 ...$, które dają najmniejszy błąd. W tym miejscu pojawia się funkcja błędu lub straty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6n-a4xDy5u_"
   },
   "source": [
    "## Funkcja straty\n",
    "Strata (koszt)  jest w zasadzie błędem w naszej przewidywanej wartości. Innymi słowy, jest to różnica między naszą przewidywaną wartością a rzeczywistą wartością. Do obliczenia błędu użyjemy [funkcji utraty L2](https://afteracademy.com/blog/what-are-l1-and-l2-loss-functions). Teoretycznie możesz użyć dowolnej funkcji do obliczenia błędu. Funkcję tę można rozłożyć jako:\n",
    "1. Niech rzeczywista wartość będzie $y_i$. Niech wartość przewidywana za pomocą naszego modelu będzie oznaczona jako $\\bar{y_i}$. Znajdź różnicę między rzeczywistą a przewidywaną wartością.\n",
    "2. Podnieś tę różnicę do kwadratu.\n",
    "3. Znajdź sumę wszystkich wartości w danych treningowych.\n",
    "$$ L = \\sum_{i = 1}^n(y_i - \\bar{y_i})^2 $$\n",
    "\n",
    "Teraz, gdy mamy błąd, musimy zaktualizować wartości naszych parametrów, aby zminimalizować ten błąd. W tym miejscu faktycznie ma miejsce „uczenie się”, ponieważ nasz model aktualizuje się na podstawie swoich poprzednich danych wyjściowych, aby uzyskać dokładniejsze dane wyjściowe w następnym kroku. Będziemy używać *Algorytmu opadania gradientu* do oszacowania naszych parametrów. Innym powszechnie stosowanym algorytmem jest [Oszacowanie maksymalnego prawdopodobieństwa](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_hesTCwy_gi"
   },
   "source": [
    "\n",
    "## Algorytm gradient descent\n",
    "\n",
    "Pochodna cząstkowa funkcji przy jej minimalnej wartości jest równa 0. Tak więc metoda spadku gradientu zasadniczo wykorzystuje tę koncepcję do oszacowania parametrów lub wag naszego modelu poprzez minimalizację funkcji straty. [Kliknij tutaj](https://www.youtube.com/watch?v=4PHI11lX11I), aby uzyskać bardziej szczegółowe wyjaśnienie, jak działa opadanie gradientu.\n",
    "Dla uproszczenia załóżmy, że nasze wyjście zależy tylko od jednej cechy $x$. Możemy więc przepisać nasze równanie jako:\n",
    "$$ \\bar{y_i} = p = \\frac{1}{1 + e^{-(b_0 + b_1x_i)}} $$\n",
    "W związku z tym musimy oszacować wartości wag $b_0$ i $b_1$ na podstawie naszych danych treningowych.\n",
    "1. Początkowo niech $b_0=0$ i $b_1=0$. Niech $L$ będzie współczynnikiem uczenia się. Szybkość uczenia się kontroluje, o ile wartości $b_0$ i $b_1$ są aktualizowane na każdym etapie procesu uczenia się. Niech $L = 0,001 $.\n",
    "2. Oblicz pochodną cząstkową względem $b_0$ i $b_1$. Wartość pochodnej cząstkowej powie nam, jak daleko jest funkcja straty od jej wartości minimalnej. Jest to miara tego, jak bardzo nasze wagi muszą zostać zaktualizowane, aby osiągnąć minimalny lub idealnie 0 błąd. Jeśli masz więcej niż jedną cechę, musisz obliczyć pochodną cząstkową dla każdej wagi $b_0$, $b_1$ ... $b_n$, gdzie $n$ to liczba cech. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aUmzO9SzFzo"
   },
   "source": [
    "## Implementing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "TNSeN-ZzybHN",
    "outputId": "3882d946-cd59-4e91-9b4d-887beb5e101b"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import exp\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"Social_Network_Ads.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "b3R547WLzqEP",
    "outputId": "2ffe111e-a546-4cbc-c697-c7df896d209c"
   },
   "outputs": [],
   "source": [
    "# Visualizing the dataset\n",
    "plt.scatter(data['Age'], data['Purchased'])\n",
    "plt.show()\n",
    "\n",
    "# Divide the data to training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Age'], data['Purchased'], test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_x2wiimzzQg"
   },
   "outputs": [],
   "source": [
    "# Creating the logistic regression model\n",
    "\n",
    "# Helper function to normalize data\n",
    "def normalize(X):\n",
    "    return X - X.mean()\n",
    "\n",
    "# Method to make predictions\n",
    "def predict(X, b0, b1):\n",
    "    return np.array([1 / (1 + exp(-1*b0 + -1*b1*x)) for x in X])\n",
    "\n",
    "# Method to train the model\n",
    "def logistic_regression(X, Y):\n",
    "\n",
    "    X = normalize(X)\n",
    "\n",
    "    # Initializing variables\n",
    "    b0 = 0\n",
    "    b1 = 0\n",
    "    L = 0.001\n",
    "    epochs = 300\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = predict(X, b0, b1)\n",
    "        D_b0 = -2 * sum((Y - y_pred) * y_pred * (1 - y_pred))  # pochodna funkcji straty dla  b0\n",
    "        D_b1 = -2 * sum(X * (Y - y_pred) * y_pred * (1 - y_pred))  # pochodna funkcji straty dla b1\n",
    "        b0 = b0 - L * D_b0\n",
    "        b1 = b1 - L * D_b1\n",
    "    \n",
    "    return b0, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "id": "CibVQeLC0WyX",
    "outputId": "e5efd281-39c3-4730-b8dc-13464f220cb0"
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "b0, b1 = logistic_regression(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "# X_test = X_test.sort_values()  # Sorting values is optional only to see the line graph\n",
    "X_test_norm = normalize(X_test)\n",
    "y_pred = predict(X_test_norm, b0, b1)\n",
    "y_pred = [1 if p >= 0.5 else 0 for p in y_pred]\n",
    "\n",
    "plt.clf()\n",
    "plt.scatter(X_test, y_test)\n",
    "plt.scatter(X_test, y_pred, c=\"red\")\n",
    "# plt.plot(X_test, y_pred, c=\"red\", linestyle='-', marker='o') # Only if values are sorted\n",
    "plt.show()\n",
    "\n",
    "# The accuracy\n",
    "accuracy = 0\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == y_test.iloc[i]:\n",
    "        accuracy += 1\n",
    "print(f\"Accuracy = {accuracy / len(y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "zdSdhARu1BLV",
    "outputId": "c28d6f05-3074-41b4-9ad1-8d7c7c43a1f7"
   },
   "outputs": [],
   "source": [
    "# Making predictions using scikit learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create an instance and fit the model \n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train.values.reshape(-1, 1), y_train.values.reshape(-1, 1))\n",
    "\n",
    "# Making predictions\n",
    "y_pred_sk = lr_model.predict(X_test.values.reshape(-1, 1))\n",
    "plt.clf()\n",
    "plt.scatter(X_test, y_test)\n",
    "plt.scatter(X_test, y_pred_sk, c=\"red\")\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "print(f\"Accuracy = {lr_model.score(X_test.values.reshape(-1, 1), y_test.values.reshape(-1, 1))}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
